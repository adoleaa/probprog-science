{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Variational Autoencoder and Generative Adversarial Networks in the context of Probabilistic Programming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we constrain ourselves to code sketches of the core ideas behind the integration of generative adversarial networks (GANs) and variational autoencoders (VAEs) with probabilistic programming frameworks.\n",
    "\n",
    "We will keep using [Turing.jl](https://github.com/TuringLang/Turing.jl), which is now combined with the machine learning stack of [Flux.jl](https://github.com/fluxml/flux.jl) to provide support for layer abstractions. This notebook basically extends upon the initial ideas from the `BayesianNeuralNetworks` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing, DifferentialEquations, DiffEqFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators: partition\n",
    "using Flux\n",
    "using Flux.Optimise: update!\n",
    "using Flux: logitbinarycrossentropy\n",
    "using Images\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Parameters: @with_kw\n",
    "using Printf\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the MNIST dataset across the GAN and the VAE example. We will start by creating a structure to hold the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct HyperParams\n",
    "    batch_size::Int = 128\n",
    "    latent_dim::Int = 100\n",
    "    epochs::Int = 20\n",
    "    verbose_freq::Int = 1000\n",
    "    output_x::Int = 6\n",
    "    output_y::Int = 6\n",
    "    lr_dscr::Float64 = 0.0002\n",
    "    lr_gen::Float64 = 0.0002\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define for ourselves a function to create output images, i.e. act in a generative fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_output_image(gen, fixed_noise, hparams)\n",
    "    @eval Flux.istraining() = false\n",
    "    fake_images = @. cpu(gen(fixed_noise))\n",
    "    @eval Flux.istraining() = true\n",
    "    image_array = permutedims(dropdims(reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_y))); dims=(3, 4)), (2, 1))\n",
    "    image_array = @. Gray(image_array + 1f0) / 2f0\n",
    "    return image_array\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sketch of GAN Inclusion in Probabilistic Programming Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin by defining for ourselves the normal GAN structure with a Discriminator and Generator. Beware that the functional syntax of Flux is quite similar to JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Discriminator()\n",
    "    Chain(Conv((4,4), 1 => 64; stride = 2, pad = 1),\n",
    "               x->leakyrelu.(x, 0.2f0),\n",
    "               Dropout(0.25),\n",
    "               Conv((4,4), 64 => 128; stride = 2, pad = 1),\n",
    "               x->leakyrelu.(x, 0.2f0),\n",
    "               Dropout(σ), \n",
    "               x->reshape(x, 7*7*128, :),\n",
    "               Dense(7*7*128, 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Generator()\n",
    "    Chain(Dense(hparams.latent_dim, 7 * 7 * 256),\n",
    "               BatchNorm(7 * 7 * 256, relu),\n",
    "               x->reshape(x, 7, 7, 256, :),\n",
    "               ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2),\n",
    "               BatchNorm(128, relu),\n",
    "               ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1),\n",
    "               BatchNorm(64, relu),\n",
    "               ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1),\n",
    "               )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining our loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function discriminator_loss(real_output, fake_output)\n",
    "    real_loss = mean(logitbinarycrossentropy.(real_output, 1f0))\n",
    "    fake_loss = mean(logitbinarycrossentropy.(fake_output, 0f0))\n",
    "    return real_loss + fake_loss\n",
    "end\n",
    "\n",
    "generator_loss(fake_output) = mean(logitbinarycrossentropy.(fake_output, 1f0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the trainings functions for the respective structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_discriminator!(gen, dscr, x, opt_dscr, hparams)\n",
    "    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size))) \n",
    "    fake_input = gen(noise)\n",
    "    ps = Flux.params(dscr)\n",
    "    # Taking gradient\n",
    "    loss, back = Flux.pullback(ps) do\n",
    "        discriminator_loss(dscr(x), dscr(fake_input))\n",
    "    end\n",
    "    grad = back(1f0)\n",
    "    update!(opt_dscr, ps, grad)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_generator!(gen, dscr, x, opt_gen, hparams)\n",
    "    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size))) \n",
    "    ps = Flux.params(gen)\n",
    "    # Taking gradient\n",
    "    loss, back = Flux.pullback(ps) do\n",
    "        generator_loss(dscr(gen(noise)))\n",
    "    end\n",
    "    grad = back(1f0)\n",
    "    update!(opt_gen, ps, grad)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this GAN we would now usually define our training function, which loops over the epochs etc. as you would expect from your machine learning framework. We provide this function here, but abstain from rewriting it for the individual subcases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(; kws...)\n",
    "    # Model Parameters\n",
    "    hparams = HyperParams(; kws...)\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    images, _ = MLDatasets.MNIST.traindata(Float32)\n",
    "    # Normalize to [-1, 1]\n",
    "    image_tensor = reshape(@.(2f0 * images - 1f0), 28, 28, 1, :)\n",
    "    # Partition into batches\n",
    "    data = [image_tensor[:, :, :, r] |> gpu for r in partition(1:60000, hparams.batch_size)]\n",
    "\n",
    "    fixed_noise = [randn(hparams.latent_dim, 1) |> gpu for _=1:hparams.output_x*hparams.output_y]\n",
    "\n",
    "    # Discriminator\n",
    "    dscr = Discriminator() |> gpu\n",
    "\n",
    "    # Generator\n",
    "    gen =  Generator() |> gpu\n",
    "\n",
    "    # Optimizers\n",
    "    opt_dscr = ADAM(hparams.lr_dscr)\n",
    "    opt_gen = ADAM(hparams.lr_gen)\n",
    "\n",
    "    # Training\n",
    "    train_steps = 0\n",
    "    for ep in 1:hparams.epochs\n",
    "        @info \"Epoch $ep\"\n",
    "        for x in data\n",
    "            # Update discriminator and generator\n",
    "            loss_dscr = train_discriminator!(gen, dscr, x, opt_dscr, hparams)\n",
    "            loss_gen = train_generator!(gen, dscr, x, opt_gen, hparams)\n",
    "\n",
    "            if train_steps % hparams.verbose_freq == 0\n",
    "                @info(\"Train step $(train_steps), Discriminator loss = $(loss_dscr), Generator loss = $(loss_gen)\")\n",
    "                # Save generated fake image\n",
    "                output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "                save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n",
    "            end\n",
    "            train_steps += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "    save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Construct a Bayesian GAN\n",
    "\n",
    "The most straightforward idea here would be to build on our ideas from the earlier notebook and turn our GAN into a Bayesian GAN, as shown in literature by [Saachti et al.](https://arxiv.org/abs/1705.09558). Akin to the probabilistic model for the Bayesian neural network we want to abstract over our architecture, which hence required us to custom-code an `unpack` function (a program writing the unpacking code would be a lot more efficient here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Discriminator_forward(xs, nn_params::AbstractVector)\n",
    "    ... = unpack(nn_params)\n",
    "    nn = Chain(Conv((...), 1 => 64; stride = 2, pad = 1),\n",
    "               x->leakyrelu.(x, 0.2f0),\n",
    "               Dropout(σ),\n",
    "               Conv((...), 64 => 128; stride = 2, pad = 1),\n",
    "               x->leakyrelu.(x, 0.2f0),\n",
    "               Dropout(σ), \n",
    "               x->reshape(x, ..., :),\n",
    "               Dense(.., 1))\n",
    "    return nn(xs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Generator_forward(xs, nn_params::AbstractVector)\n",
    "    ... = unpack(nn_params)\n",
    "    nn = Chain(Dense(hparams.latent_dim, 7 * 7 * 256),\n",
    "               BatchNorm(7 * 7 * 256, relu),\n",
    "               x->reshape(x, 7, 7, 256, :),\n",
    "               ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2),\n",
    "               BatchNorm(128, relu),\n",
    "               ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1),\n",
    "               BatchNorm(64, relu),\n",
    "               ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1),\n",
    "               )\n",
    "    return nn(xs)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have constructed our forward function we need to specify our probabilistic models. We follow the syntax of `BayesianNeuralNetworks` notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regularization term and a Gaussian prior variance term\n",
    "alpha = 0.09\n",
    "sig = sqrt(1.0 / alpha)\n",
    "\n",
    "# Specify the probabilistic model for the discriminator\n",
    "@model bayes_discriminator(xs, ts) = begin\n",
    "    # Create the weight and bias vector\n",
    "    nn_params ~ MvNormal(zeros(20), sig .* ones(20))\n",
    "    \n",
    "    # Calculate predictions for the inputs given the weights and biases in theta\n",
    "    preds = Discriminator_forward(xs, nn_params)\n",
    "    \n",
    "    # Observe each prediction\n",
    "    for i = 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;\n",
    "\n",
    "# Specify the probabilistic model for the generator\n",
    "@model bayes_generator(xs, ts) = begin\n",
    "    # Create the weight and bias vector\n",
    "    nn_params ~ MvNormal(zeros(20), sig .* ones(20))\n",
    "    \n",
    "    # Calculate predictions for the inputs given the weights and biases in theta\n",
    "    preds = Generator_forward(xs, nn_params)\n",
    "    \n",
    "    # Observe each prediction\n",
    "    for i = 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference we would now have to construct a third `@model`, i.e. create a higher-order structure which inherits from the two probabilistic models above, which makes for a very involved inference problem - also expensive - but it should not be outright intractable. A big issue here would be the instability of the training procedure though, which could lead our samplers to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train a Generative Model Emulator\n",
    "\n",
    "Akin to inference compilation we can also train a GAN to fulfill that role. This would not be entirely automated, but once the generator is trained on the model it should be highly useful as a surrogate to then be used in either inference routines, model-based optimization, model-based reinforcement learning etc. Expanding upon the language for yesterday's lecture, we essentially seek to condition the GAN on the observations of our actual model. For this we'd essentially need to only modify the training procedure and the helper function pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of allowed samples\n",
    "N = 10000\n",
    "\n",
    "function train(; kws...)\n",
    "    # Model Parameters\n",
    "    hparams = HyperParams(; kws...)\n",
    "    \n",
    "    # Load probabilistic model data\n",
    "    trainings_samples = sample(example_model(), HMC(0.05, 4), N)\n",
    "    # Partition trainings data into batches\n",
    "    data = [trainings_samples[:, :, :, r] |> gpu for r in partition(110000, hparams.batch_size)]\n",
    "\n",
    "    fixed_noise = [randn(hparams.latent_dim, 1) |> gpu for _=1:hparams.output_x*hparams.output_y]\n",
    "\n",
    "    # Discriminator\n",
    "    dscr = Discriminator() |> gpu\n",
    "\n",
    "    # Generator\n",
    "    gen =  Generator() |> gpu\n",
    "\n",
    "    # Optimizers\n",
    "    opt_dscr = ADAM(hparams.lr_dscr)\n",
    "    opt_gen = ADAM(hparams.lr_gen)\n",
    "\n",
    "    # Training\n",
    "    train_steps = 0\n",
    "    for ep in 1:hparams.epochs\n",
    "        @info \"Epoch $ep\"\n",
    "        for x in data\n",
    "            # Update discriminator and generator\n",
    "            loss_dscr = train_discriminator!(gen, dscr, x, opt_dscr, hparams)\n",
    "            loss_gen = train_generator!(gen, dscr, x, opt_gen, hparams)\n",
    "\n",
    "            if train_steps % hparams.verbose_freq == 0\n",
    "                @info(\"Train step $(train_steps), Discriminator loss = $(loss_dscr), Generator loss = $(loss_gen)\")\n",
    "                # Save generated samples\n",
    "                generated_sample = create_output_sample(gen, fixed_noise, hparams)\n",
    "                save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), generated_sample)\n",
    "            end\n",
    "            train_steps += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    generated_sample = create_output_sample(gen, fixed_noise, hparams)\n",
    "    save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), generated_sample)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where quite a bit of the surrounding boilerplate code is missing. But after training we would then be able to use the generator as a surrogate model in our pipeline, e.g. for HMC to sample from to save computational costs. We just need to stay cautious that the sampler does not try to sample from regions, where the surrogate has no \"support\" and would hence be generalizing in that region - a feat that would be quite remarkable for current machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Abstract the GAN as a Probabilistic Program\n",
    "\n",
    "Viewing the GAN as its own probabilistic program incurs the highest syntactical overhead as, recalling our introduction to higher-order probabilistic programming languages and Turing yesterday, we need to custom-define our neural network layers to act in a distributional sense. This is akin to the Bayesian construction but given a less-constrained stochastic control flow faces a lot fewer programmatic constraints and can even be viewed in a similar light as a neural architecture search.\n",
    "\n",
    "To begin with such a task we have to define the custom types for our architecture. Where it makes the most sense to decompose the GAN, using the generator as the example here into different blocks, for which we then define our custom distribution structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Generator()\n",
    "    Chain(Dense(hparams.latent_dim, 7 * 7 * 256),\n",
    "               BatchNorm(7 * 7 * 256, relu),\n",
    "               x->reshape(x, 7, 7, 256, :),\n",
    "               ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2),\n",
    "               BatchNorm(128, relu),\n",
    "               ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1),\n",
    "               BatchNorm(64, relu),\n",
    "               ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1),\n",
    "               )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classical split-up would in this case `ConvTranspose` together with the respective `BatchNorm` with the two other custom blocks being `Dense` and `BatchNorm`, `reshape` and `ConvTranspose`. Following the customized distribution definition for Turing we then have to first establish the structures, taking one as an example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct custom_nn_layer <: Multinomial\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to define the sampling and evaluation of the log-pdf of our custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distributions.rand(rng::AbstractRNG, d:custom_nn_layer) = ...\n",
    "Distributions.logpdf(d::custom_nn_layer, x::Real) = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the computational complexity of neural network layers we would, in addition to the bijection definition, also have to vectorize the operatiors as inference would otherwise be intractable. We could then reassemble our generator structure as its own model with the unshackled stochastic control flow (there would be a few more difficulties to such use :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function unshackled_Generator() = begin\n",
    "        Chain(custom_nn_layer_1(..),\n",
    "            custom_nn_layer_2(..),\n",
    "            custom_nn_layer_3(..),\n",
    "            custom_nn_layer_3(..)\n",
    "        )\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now have defined a highly flexible stochastic control flow, which would open up new opportunities on the architectural front. But also present a highly challenging inference problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketch of VAE Inclusion in Probabilistic Programming Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a VAE like a probabilistic model we first need to repeat the same steps as for the GAN and write down the supporting functions. Assuming that we still have the MNIST data loading pipeline from the GAN example we begin by establishing our encoder and decoder structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Encoder\n",
    "    linear\n",
    "    μ\n",
    "    logσ\n",
    "    Encoder(input_dim, latent_dim, hidden_dim, device) = new(\n",
    "        Dense(input_dim, hidden_dim, tanh) |> device,   # linear\n",
    "        Dense(hidden_dim, latent_dim) |> device,        # μ\n",
    "        Dense(hidden_dim, latent_dim) |> device,        # logσ\n",
    "    )\n",
    "end\n",
    "\n",
    "function (encoder::Encoder)(x)\n",
    "    h = encoder.linear(x)\n",
    "    encoder.μ(h), encoder.logσ(h)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder(input_dim, latent_dim, hidden_dim, device) = Chain(\n",
    "    Dense(latent_dim, hidden_dim, tanh),\n",
    "    Dense(hidden_dim, input_dim)\n",
    ") |> device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reconstruction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reconstuct(encoder, decoder, x, device)\n",
    "    μ, logσ = encoder(x)\n",
    "    z = μ + device(randn(Float32, size(logσ))) .* exp.(logσ)\n",
    "    μ, logσ, decoder(z)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(encoder, decoder, λ, x, device)\n",
    "    μ, logσ, decoder_z = reconstuct(encoder, decoder, x, device)\n",
    "    len = size(x)[end]\n",
    "    # KL-divergence\n",
    "    kl_q_p = 0.5f0 * sum(@. (exp(2f0 * logσ) + μ^2 -1f0 - 2f0 * logσ)) / len\n",
    "\n",
    "    logp_x_z = -sum(logitbinarycrossentropy.(decoder_z, x)) / len\n",
    "    # regularization\n",
    "    reg = λ * sum(x->sum(x.^2), Flux.params(decoder))\n",
    "    \n",
    "    -logp_x_z + kl_q_p + reg\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As have both, our encoder and decoder now we can define our probabilistic model (absolute sketches at this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model vae_model(..) = begin\n",
    "    z ~ Normal(..) # sample from the prior\n",
    "    decoder(z)\n",
    "    # score against actual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model guide(..) = begin\n",
    "    z1, z2 = encoder(..)\n",
    "    Normal(z1, z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training routine then looks along the lines of the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hyperparamters\n",
    "args = Args(; kws...)\n",
    "args.seed > 0 && Random.seed!(args.seed)\n",
    "    \n",
    "device = cpu\n",
    "\n",
    "# load MNIST images\n",
    "loader = get_data(args.batch_size)\n",
    "    \n",
    "# initialize encoder and decoder\n",
    "encoder = Encoder(args.input_dim, args.latent_dim, args.hidden_dim, device)\n",
    "decoder = Decoder(args.input_dim, args.latent_dim, args.hidden_dim, device)\n",
    "\n",
    "# ADAM optimizer\n",
    "opt = ADAM(args.η)\n",
    " \n",
    "# fixed input\n",
    "original, _ = first(get_data(args.sample_size^2))\n",
    "original = original |> device\n",
    "image = convert_to_image(original, args.sample_size)\n",
    "image_path = joinpath(args.save_path, \"original.png\")\n",
    "save(image_path, image)\n",
    "\n",
    "# Configure ADVI\n",
    "advi = ADVI(10, 10_000)\n",
    "\n",
    "# training\n",
    "train_steps = 0\n",
    "@info \"Start Training, total $(args.epochs) epochs\"\n",
    "for epoch = 1:args.epochs\n",
    "    @info \"Epoch $(epoch)\"\n",
    "    progress = Progress(length(loader))\n",
    "\n",
    "    for (x, _) in loader \n",
    "        q = vi(vae_model, guide)\n",
    "        epoch_loss += AdvancedVI.elbo()\n",
    "    end\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
