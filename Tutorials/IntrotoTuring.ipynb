{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Depth Introduction to Turing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an in-depth introduction to the [Turing.jl](https://turing.ml) a high-level probabilistic programming language written in [Julia](https://julialang.org). We will begin by building a simple model and then exploring the inference interface, before exploring the backend syntax of `@model`, the kinks and quirks of the automatic differentiation backend, how to express our probabilistic models performantly and visualizing our results\n",
    "\n",
    "The tutorial is based on the [Turing tutorials](https://turing.ml/dev/docs/using-turing) and material from the [Zygote.jl](https://github.com/FluxML/Zygote.jl) [documentation](https://fluxml.ai/Zygote.jl/dev/) on automatic differentiation.\n",
    "\n",
    "\n",
    "## Outline:\n",
    "\n",
    "**Section 1** [Starting in Turing](#starting)\n",
    "\n",
    "**Section 2** [Modelling Syntax](#modelling-syntax)\n",
    "\n",
    "**Section 3** [Sampling with Multiple Chains](#multiple-chains)\n",
    "\n",
    "**Section 4** [The Chain as a Data Structure](#chain-data-struct)\n",
    "\n",
    "**Section 5** [Maximum Likelihood and Maximum a Posterior Estimates](#mle-map)\n",
    "\n",
    "**Section 6** [Advanced Interface](#advanced)\n",
    "\n",
    "**Section 7** [Automatic Differentation](#ad)\n",
    "\n",
    "**Section 8** [Performance Optimization](#perf-opt)\n",
    "\n",
    "**Section 9** [Sampler Visualization](#sampler-viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting in Turing <a name=\"starting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using StatsPlots\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions of random variables are defined by `x ~ distribution`. If `x` is already defined Turing views this as us conditioning on `x` having been drawn from this distribution. The likelihood is always computed using the `logpdf`.\n",
    "\n",
    "We begin by defining a normal model with unknown mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function gdemo(x, y)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    x ~ Normal(m, sqrt(s))\n",
    "    y ~ Normal(m, sqrt(s))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference is performed using the `sample` statement to which we provide our probabilistic program as the first argument, and the sampler as the second argument. The available samplers are:\n",
    "* Gibbs-sampler\n",
    "* Hamiltonian Monte-Carlo sampler\n",
    "* Hamiltonian Monte-Carlo sampler with Dual Averaging algorithm\n",
    "* Importance sampling\n",
    "* No-U-Turn sampler\n",
    "* Particle Gibbs sampler\n",
    "* Sequential Monte Carlo sampler\n",
    "\n",
    "Using the `DynamicPPL.Sampler` interface we are furthermore able to implement our own inference algorithms for which we need to specify the type of the algorithm and parameters with inheritance from `InferenceAlgorithm` and a `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = sample(gdemo(1.5, 2), SMC(), 1000)\n",
    "c2 = sample(gdemo(1.5, 2), PG(10), 1000)\n",
    "c3 = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)\n",
    "c4 = sample(gdemo(1.5, 2), Gibbs(PG(10, :m), HMC(0.1, 5, :s)), 1000)\n",
    "c5 = sample(gdemo(1.5, 2), HMCDA(0.15, 0.65), 1000)\n",
    "c6 = sample(gdemo(1.5, 2), NUTS(0.65), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the APIs for each of the inference algorithms is defined as:\n",
    "\n",
    "### Gibbs-Sampler\n",
    "\n",
    "Args:\n",
    "* multiple inference algorithms, which to be combined by the Gibbs sampler. This allows us to use particle-based methods, like `PG` for discrete variables and `HMC` for continuous variables.\n",
    "\n",
    "### Hamiltonian Monte-Carlo\n",
    "\n",
    "Args:\n",
    "* leapfrog step size to use, which is to be reduced when receiving gradient errors\n",
    "* number of leapfrog steps to use\n",
    "\n",
    "### Hamiltonian Monte-Carlo Dual-Averaging\n",
    "\n",
    "Args:\n",
    "* Number of samples to use for adaptation\n",
    "* Target acceptance rate, a good guess is usually 65%\n",
    "* Target leapfrog length\n",
    "* Initial step size, if the step size is 0 Turing automatically searches for the optimal step size\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "Args:\n",
    "* Requires no arguments\n",
    "\n",
    "### No-U-Turn Sampler\n",
    "\n",
    "Args:\n",
    "* Number of samples to use with adaptation\n",
    "* Target acceptance rate for dual averaging\n",
    "* Maximum doubling tree depth\n",
    "* Maximum divergence during double tree\n",
    "* Initial step size, if the step size is 0 Turing automatically searches for the optimal step size\n",
    "\n",
    "### Particle-Gibbs\n",
    "\n",
    "Args:\n",
    "* Number of particles\n",
    "* Optionally a custom resampling algorithm\n",
    "\n",
    "### Sequential Monte Carlo\n",
    "\n",
    "Args:\n",
    "* Requires no arguments, but can optionally specify a custom resampling algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing summary statistics for one of the sampled chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Syntax <a name=\"moelling-syntax\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order in which we write out the model code matters. In this example `s` needs to be declared before `y`, as `y`\n",
    "uses the sampled result from `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function example(y)\n",
    "    s ~ Poisson(1)\n",
    "    y ~ Normal(s, 1)  # \n",
    "    return y\n",
    "end\n",
    "\n",
    "sample(example(10), SMC(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with Multiple Chains <a name=\"multiple-chains\"></a>\n",
    "\n",
    "With Turing we employ threaded, as well as parallel sampling to fully exploit the computational resources available to use. This is defined in the `sample` statement with an extra argument inside of the call, i.e. `sample(model, sampler, {MCMCThreads(), MCMCDistributed()}, n, n_chains)`. With multiple chains we can then evaluate convergence characteristics. If parallelism is not desired, one can still sample multiple chains through the `mapreduce` function.\n",
    "\n",
    "As the `chains` variable now contains multiple chains, which can be indexed by chain, e.g. for the first chain `chains[:, :, 1]`.\n",
    "\n",
    "For multithreaded sampling with 4 chains in this example, for which the Julia instance needs to be started with multiple threads to actually \"fill\" those threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function multithreading_demo(x)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "end\n",
    "\n",
    "model = multithreading_demo([1.5, 2.0])\n",
    "\n",
    "chain = sample(model, NUTS(), MCMCThreads(), 1000, 4; save_state=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using distributed sampling with four sampling parallel sampling processes and enforcing a precompiled Turing on all processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)\n",
    "\n",
    "@everywhere using Turing\n",
    "\n",
    "@everywhere @model function distributed_demo(x)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere model = distributed_demo([1.5, 2.0])\n",
    "\n",
    "#sample(model, NUTS(), MCMCDistributed(), 1000, 4) --> Takes way too long, bug?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from a model's prior we can simply generate a chain out of the prior, i.e. `chain = sample(model, Prior(), n_samples)`. We are furthermore able to run our model from the prior distribution, by calling the model without specifying inputs or a sampler. Our example model returns two variables, but also takes two variables as input. Not specifying the two leads Turing to believe that they are missing values, which are to be drawn from the respective distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function unconditional_demo(x, y)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    x ~ Normal(m, sqrt(s))\n",
    "    y ~ Normal(m, sqrt(s))\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "unconditional_prior_sample = unconditional_demo(missing, missing)\n",
    "unconditional_prior_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from a model's posterior, i.e. treat observations as random variables we have to express our model\n",
    "with the following syntax, where the missing variable needs to be initialized explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function conditional_demo(x, ::Type{T} = Float64) where {T}\n",
    "    if x === missing\n",
    "        # Initialize `x` if missing\n",
    "        x = Vector{T}(undef, 2)\n",
    "    end\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Construct a model with x = missing\n",
    "model = gdemo(missing)\n",
    "c = sample(model, HMC(0.01, 5), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use a mixture of `missing` and non-`missing` values in the same random variable `x`. The missing entries will be treated as random variables to be sampled, while non-`missing` values get treated as observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function conditional_missing_demo(x)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "end\n",
    "\n",
    "model = conditional_missing_demo([missing, 2.4])\n",
    "c = sample(model, HMC(0.01, 5), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat an observation by default as a random variable, we can specify such trait in the model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function conditional_default_demo(x = missing, ::Type{T} = Float64) where {T <: Real}\n",
    "    if x === missing\n",
    "        x = Vector{T}(undef, 10)\n",
    "    end\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in 1:length(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "    return s, m\n",
    "end\n",
    "\n",
    "m = conditional_default_demo()\n",
    "chain = sample(m, HMC(0.01, 5), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chain as a Datastructure <a name=\"chain-data-struct\"></a>\n",
    "\n",
    "Values stored inside of a chain can be accessed in multiple ways:\n",
    "\n",
    "1. Transform them into `DataFrames`\n",
    "2. Use their raw `AxisArray` form\n",
    "3. Create a three-dimensional `Array` object\n",
    "\n",
    "When defining the variable types we need to abide by a few rules:\n",
    "\n",
    "1. When using the Hamiltonian sampler we should use `real` values to enable auto-differentiation through the model\n",
    "2. When using a particle sampler variables should preferably be 'TArray's\n",
    "3. Use the type parameter definition in the model header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Probabilities from our Model, or our Chain\n",
    "\n",
    "When having a demo model we can query the model and its likelihoods in a multitude of ways using the `prob` command, if we are interested in log-probabilities you just need to replace `prob` with `logprob` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function probability_query_demo(x, y)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    x ~ Normal(m, sqrt(s))\n",
    "    y ~ Normal(m, sqrt(s))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the likelihood of `x=1`and `y=1` given `s=1` and `m=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob\"x=1.0, y=1.0 | model=probability_query_demo, s=1.0, m=1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the joint probability of `s=1` and `m=1` ignoring `x` and `y`, so they can optionally be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob\"s=1.0, m=1.0 | model=probability_query_demo, x=nothing, y=nothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the joint probability of `s=1`, `m=1` and `x=1` ignoring `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob\"s=1.0, m=1.0, x=1.0 | model=probability_query_demo, y=nothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the joint probability of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob\"s=1.0, m=1.0, x=1.0, y=1.0 | model=probability_query_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for chains after MCMC sampling has taken place. E.g. calculating the element-wise likelihood of `x=1.0` and `y=1.0` for each sample in the chain `prob\"x=1.0, y=1.0 | chain=chain, model=multithreading_demo\"` or `prob\"x=1.0, y=1.0 | chain=chain\"` if `save_state=true` was set at sampling time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood and Maximum a Posterior Estimates <a name=\"mle-map\"></a>\n",
    "\n",
    "For mode estimation Turing provides maximum likelihood estimation (MLE) and maximum a posterior (MAP) estimation. To benefit from these capabilities, we need to load the `Optim` package. For mode estimation to work all parameters furthermore need to be continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function mode_demo(x)\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1.5, 2.0]\n",
    "model = mode_demo(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefitting from `Optim.optimize` Turing accepts `MLE()` or `MAP`. The optimizer uses LBFGS by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_estimate = optimize(model, MLE())\n",
    "map_estimate = optimize(model, MAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer can be altered by using the optional third argument and putting an alternative such as\n",
    "\n",
    "* NelderMead\n",
    "* SimulatedAnnealing\n",
    "* ParticleSwarm\n",
    "* Newton\n",
    "* AcceleratedGradientDescent\n",
    "\n",
    "If Optim fails to converge, it is prudent to increase the number of possible `iterations` or allow for steps that increase the objective value by setting `allow_f_increase` to true in an additional argument. This would then look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_estimate = optimize(model, MLE(), Newton(), Optim.Options(iterations=10_000, allow_f_increases=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turing benefits from inheritance from the `StatsBase` package, which provides analysis tools to dissect the mode estimation results. The most prevalent of which are coefficient-tables (`coeftable`), and the Fisher information matrix (`informationmatrix`). E.g. using the coefficient table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeftable(mle_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also begin sampling a chain from the MLE/MAP estimate. For this we have to extract the vector of parameter values and provide it to the `sample` function to `init_theta`. E.g. sampling from the full posterior using the MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate = optimize(model, MAP())\n",
    "chain = sample(model, NUTS(), 1_000, init_theta=map_estimate.values.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics of Chains using MCMCChains.jl\n",
    "\n",
    "Turing's samples are wrapped by `MCMCChains.Chain`, hence allowing the analysis of chains using all available diagnostics from [MCMCChains](https://github.com/TuringLang/MCMCChains.jl). Examples of available functionality include, but are not limited to:\n",
    "\n",
    "Convergence diagnostics such as:\n",
    "* Gelman, Rubin, and Brooks Diagnostics\n",
    "* Geweke Diagnostics\n",
    "* Heidelberger and Welch Diagnostics\n",
    "* Raftery and Lewis Diagnostics\n",
    "\n",
    "Model selection with the *Deviance Information Criterion (DIC)*, plotting support and multiple ways to export chains into `Array` structures or `DataFrame` structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Interfaces <a name=\"advanced\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Turing support a wide array of distributions through the [Distributions.jl](https://github.com/JuliaStats/Distributions.jl) package, it allows for the custom definition of distributions by benefitting from subtypes of `Distribution`, which then have to be complemented with corresponding functions. E.g. if we were to define a uniform distribution it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CustomUniform <: ContinuousUnivariateDistribution\n",
    "end\n",
    "\n",
    "# Define rand and logpdf\n",
    "Distributions.rand(rng::AbstractRNG, d::CustomUniform) = rand(rng)\n",
    "Distributions.logpdf(d::CustomUniform, x::Real) = zero(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `HMC` requires the domain of priors to be unbounded we furthermore need to define a bijector from `[0, 1]` to `ℝ`. For this we have to use the [`Bijectors.jl`](https://github.com/TuringLang/Bijectors.jl) package, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bijectors.bijector(d::CustomUniform) = Logit(0., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the uniform case we furthermore want to define the minimum and maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distributions.minimum(d::CustomUniform) = 0.\n",
    "Distributions.maximum(d::CustomUniform) = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance-oriented implementations it is furthermore recommended to implement vectorization support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@model` generates a `Turing.Model` structure, which is then re-used by the sampler. We will now examine what the `gdemo` example translates into when the macro is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model gdemo(x) = begin\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    \n",
    "    # Observe each value of x\n",
    "    @. x ~ Normal(m, sqrt(s))\n",
    "end\n",
    "\n",
    "sample(gdemo([1.5, 2.0]), HMC(0.1, 5), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a NamedTuple containing the data variables\n",
    "data = (x = [1.5, 2.0],)\n",
    "\n",
    "# Create model function\n",
    "mf(vi, sampler, ctx, model) = begin\n",
    "    # Set the accumulated logp to zero\n",
    "    resetlogp!(vi)\n",
    "    x = model.args.x\n",
    "    \n",
    "    # Assume s has an InverseGamma distribution\n",
    "    s, lp = Turing.Inference.tilde(\n",
    "        ctx,\n",
    "        sampler,\n",
    "        InverseGamma(2, 3),\n",
    "        Turing.@varname(s),\n",
    "        (),\n",
    "        vi,\n",
    "    )\n",
    "    \n",
    "    # Add the lp to the accumullated logp\n",
    "    acclogp!(vi, lp)\n",
    "    \n",
    "    # Assume m has a Normal distribution\n",
    "    m, lp = Turing.Inference.tilde(\n",
    "        ctx,\n",
    "        sampler,\n",
    "        Normal(0, sqrt(s)),\n",
    "        Turing.@varname(m),\n",
    "        (),\n",
    "        vi,\n",
    "    )\n",
    "    \n",
    "    # Add the lp to the accumulated logp\n",
    "    acclogp!(vi, lp)\n",
    "    \n",
    "    # Observe each value of x[i], according to a\n",
    "    # Normal distribution\n",
    "    lp = Turing.Inference.dot_tilde(ctx, sampler, Normal(m, sqrt(s)), x, vi)\n",
    "    acclogp!(vi, lp)\n",
    "end\n",
    "\n",
    "# Instantiate the model object\n",
    "model = DynamicPPL.Model(mf, data, DynamicPPL.ModelGen{()}(nothing, nothing))\n",
    "\n",
    "# Sample the model\n",
    "chain = sample(model, HMC(0.1, 5), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation <a name=\"ad\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zygote's core ingredient is the definition of the pullback, which defines the adjoint for the computational operation. All automatic differentiation frameworks, have to perform this step on some level. Looking at this in Zygote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, back = Zygote.pullback(sin, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`back` implements the pullback, which is the gradient computation for `sin` here. It implements a vector-Jacobian product, where ``y = f(x)`` and the gradient is written ``x-bar``, the pullback ``\\mathcal{B}_y`` then computer the partial of l over the partial of x, which is the same as the partial of l w.r.t. y times the partial of y w.r.t. x, which is the same as the pullback.\n",
    "\n",
    "`pullback(sin, x)` is hence the same as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsin(x) = (sin(x), ȳ -> (ȳ * cos(x),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which generally expressed then amounts to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mygradient(f, x...)\n",
    "    _, back = Zygote.pullback(f, x...)\n",
    "    back(1)\n",
    "end\n",
    "\n",
    "mygradient(sin, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compositional Sampling with Differing AD Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Julia there currently exist a number of different automatic differentiation backends, the most famous of which `Zygote.jl` is viewed as the future, but has so far not come out of alpha-stage yet. This has hence lead to most performant codes relying on a mixture of automatic differentiation frameworks. Turing, as well as Gen for that matter, does by default rely on `ForwardDiff`, but has the ability to switch its backend to `Tracker`, `Zygote` or `ReverseDiff` who can incur their own performance penalties, but mixed-use is also possible.\n",
    "\n",
    "One would switch the backend by invoking `Turing.setadbackend({:forwarddiff, :tracker, :zygote, :reversediff})`\n",
    "\n",
    "Attention: `ReverseDiff` invites the use of [memoization](https://en.wikipedia.org/wiki/Memoization), caching already sampled values for later reuse, which can result in incorrect results or crashes in some probabilistic programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model compositional_sampling_demo(x, y) = begin\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    x ~ Normal(m, sqrt(s))\n",
    "    y ~ Normal(m, sqrt(s))\n",
    "end\n",
    "\n",
    "c = sample(\n",
    "    compositional_sampling_demo(1.5, 2),\n",
    "    Gibbs(\n",
    "        HMC{Turing.ForwardDiffAD{1}}(0.1, 5, :m),\n",
    "        HMC{Turing.TrackerAD}(0.1, 5, :s)\n",
    "    ),\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different backends all have their own advantages, which can used to boost performance. `TrackerAD` is best-suited for high dimensional problems, whereas `ForwardDiffAD` shines on lower-dimensional variables. The default backend is `ForwardDiffAD`. If you decide to use `Tracker` or `Zygote` loops should be avoided at all costs, as they incur heavy performance penalties. Loops can be avoided by utilizing matrix-variate distributions or by writing custom distribution with corresponding custom adjoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization <a name=\"perf-opt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops inside of the generative model should be avoided, try to replace loops with multivariate distributions when possible. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model multivariate_demo(x) = begin\n",
    "    m ~ Normal()\n",
    "    for i = 1:length(x)\n",
    "        x[i] ~ Normal(m, 0.2)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model multivariate_demo(x) = begin\n",
    "    m ~ Normal()\n",
    "    x ~ MvNormal(fill(m, length(x)), 0.2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure type-stability of your probabilistic models. This ensures that Julia always returns a value of the same type as output, improves performance i.e. how many functions does Julia need to compile for an expression? It furthermore makes life easier for the inference algorithms. `@code_warntype` can furthermore unearth type instabilities in the model definitions.\n",
    "\n",
    "Not type-stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model tmodel(x, y) = begin\n",
    "    p, n = size(x)\n",
    "    params = Vector{Real}(undef, n)\n",
    "    for i = 1:n\n",
    "        params[i] ~ truncated(Normal(), 0, Inf)\n",
    "    end\n",
    "    a = x * params\n",
    "    y ~ MvNormal(a, 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type-stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model tmodel(x, y, ::Type{T}=Vector{Float64}) where {T} = begin\n",
    "    p, n = size(x)\n",
    "    params = T(undef, n)\n",
    "    for i = 1:n\n",
    "        params[i] ~ truncated(Normal(), 0, Inf)\n",
    "    end\n",
    "    a = x * params\n",
    "    y ~ MvNormal(a, 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for an effective use of `@code_warntype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model tmodel(x) = begin\n",
    "    p = Vector{Real}(undef, 1);\n",
    "    p[1] ~ Normal()\n",
    "    p = p .+ 1\n",
    "    x ~ Normal(p[1])\n",
    "end\n",
    "\n",
    "model = tmodel(1.0)\n",
    "varinfo = Turing.VarInfo(model)\n",
    "spl = Turing.SampleFromPrior()\n",
    "\n",
    "@code_warntype model.f(model, varinfo, spl, Turing.DefaultContext())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core concept, which when used properly can be highly benefitial in scientific computing application is the one of memoization, for which we need use [Memoization.jl](https://github.com/marius311/Memoization.jl) in Julia with which we can then cache expensive function evaluations at inference time, especially in the context Gibbs sampling with all of its sub-iterations. Memoization essentially stores a function execution, so we we can reuse the output for every input it has already been evaluated with. For this we define memoized version of the function executions, which we can potentially identify through profiling.\n",
    "\n",
    "Non-memoized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model memoization_demo(x) = begin\n",
    "    a ~ Gamma()\n",
    "    b ~ Normal()\n",
    "    c = function1(a)\n",
    "    d = function2(b)\n",
    "    x .~ Normal(c, d)\n",
    "end\n",
    "\n",
    "alg = Gibbs(MH(:a), MH(:b))\n",
    "sample(memoization_demo(zeros(10)), alg, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memoized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Memoization\n",
    "\n",
    "# define memoized function versions\n",
    "@memoize memoized_function1(args...) = function1(args...)\n",
    "@memoize memoized_function2(args...) = function2(args...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model memoized_demo(x) = begin\n",
    "    a ~ Gamma()\n",
    "    b ~ Normal()\n",
    "    c = memoized_function1(a)\n",
    "    d = memoized_function2(b)\n",
    "    x .~ Normal(c, d)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Visualization <a name=\"sampler-viz\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a plotting function to visualize the sampler paths by plotting the sampler's trajectory across the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"GKS_ENCODING\"] = \"utf-8\"\n",
    "using Plots\n",
    "using StatsPlots\n",
    "using Turing\n",
    "using Bijectors\n",
    "using Random\n",
    "using DynamicPPL: getlogp, settrans!, getval, reconstruct, vectorize, setval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "Random.seed!(0)\n",
    "\n",
    "@model viz_demo(x) = begin\n",
    "    s ~ InverseGamma(2, 3)\n",
    "    m ~ Normal(0, sqrt(s))\n",
    "    bumps = sin(m) + cos(m)\n",
    "    m = m + 5*bumps\n",
    "    for i in eachindex(x)\n",
    "        x[i] ~ Normal(m, sqrt(s))\n",
    "    end\n",
    "    return s, m\n",
    "end\n",
    "\n",
    "# Define data points\n",
    "x = [1.5, 2.0, 13.0, 2.1, 0.0]\n",
    "\n",
    "model = viz_demo(x)\n",
    "vi = Turing.VarInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variance parameters to the real prior to sampling\n",
    "dist = InverseGamma(2,3)\n",
    "svn = vi.metadata.s.vns[1]\n",
    "mvn = vi.metadata.m.vns[1]\n",
    "setval!(vi, vectorize(dist, Bijectors.link(dist, reconstruct(dist, getval(vi, svn)))), svn)\n",
    "settrans!(vi, true, svn)\n",
    "\n",
    "# Evaluate surface at coordinates\n",
    "function evaluate(m1, m2)\n",
    "    spl = Turing.SampleFromPrior()\n",
    "    vi[svn] = [m1]\n",
    "    vi[mvn] = [m2]\n",
    "    model(vi, spl)\n",
    "    getlogp(vi)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_sampler(chain; label=\"\")\n",
    "    # Extract values from chain.\n",
    "    val = get(chain, [:s, :m, :lp])\n",
    "    ss = link.(Ref(InverseGamma(2, 3)), val.s)\n",
    "    ms = val.m\n",
    "    lps = val.lp\n",
    "\n",
    "    # How many surface points to sample.\n",
    "    granularity = 100\n",
    "\n",
    "    # Range start/stop points.\n",
    "    spread = 0.5\n",
    "    σ_start = minimum(ss) - spread * std(ss); σ_stop = maximum(ss) + spread * std(ss);\n",
    "    μ_start = minimum(ms) - spread * std(ms); μ_stop = maximum(ms) + spread * std(ms);\n",
    "    σ_rng = collect(range(σ_start, stop=σ_stop, length=granularity))\n",
    "    μ_rng = collect(range(μ_start, stop=μ_stop, length=granularity))\n",
    "\n",
    "    # Make surface plot.\n",
    "    p = surface(σ_rng, μ_rng, evaluate,\n",
    "          camera=(30, 65),\n",
    "        #   ticks=nothing,\n",
    "          colorbar=false,\n",
    "          color=:inferno,\n",
    "          title=label)\n",
    "\n",
    "    line_range = 1:length(ms)\n",
    "\n",
    "    scatter3d!(ss[line_range], ms[line_range], lps[line_range],\n",
    "        mc =:viridis, marker_z=collect(line_range), msw=0,\n",
    "        legend=false, colorbar=false, alpha=0.5,\n",
    "        xlabel=\"σ\", ylabel=\"μ\", zlabel=\"Log probability\",\n",
    "        title=label)\n",
    "\n",
    "    return p\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, Gibbs(HMC(0.01, 5, :s), PG(20, :m)), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, HMC(0.01, 10), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte-Carlo with Dual Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, HMCDA(200, 0.65, 0.3), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No U-Turn Sampler (NUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, NUTS(0.65), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, NUTS(0.95), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, NUTS(0.2), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Gibbs sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, PG(20), 1000)\n",
    "plot_sampler(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sample(model, PG(50), 1000)\n",
    "plot_sampler(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
